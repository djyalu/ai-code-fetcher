import { AIModel } from '@/types/chat';

export const MODELS: AIModel[] = [
  // ============= Premium Models - Frontier =============
  {
    id: 'gpt-5.2',
    name: 'GPT-5.2',
    provider: 'openai',
    description: 'ìµœê³  ì„±ëŠ¥. Agentic Workflow ë° ì´ˆìž¥ë¬¸ ë§¥ë½ ì´í•´.',
    inputPrice: 10,
    outputPrice: 30,
    contextWindow: 128000,
    color: '#10a37f',
  },
  {
    id: 'gpt-5.2-codex',
    name: 'GPT-5.2 Codex',
    provider: 'openai',
    description: 'ì½”ë”© ìµœê°•. ë³µìž¡í•œ ì—”ì§€ë‹ˆì–´ë§ ë° ì•„í‚¤í…ì²˜ ì„¤ê³„ìš©.',
    inputPrice: 15,
    outputPrice: 45,
    contextWindow: 128000,
    color: '#10a37f',
  },
  {
    id: 'claude-3-5-sonnet',
    name: 'Claude 3.5 Sonnet',
    provider: 'anthropic',
    description: 'ìžì—°ìŠ¤ëŸ¬ìš´ ë¬¸ìž¥ë ¥, ì½”ë”©, ì¶”ë¡  ë°¸ëŸ°ìŠ¤ê°€ ê°€ìž¥ ì¢‹ìŒ.',
    inputPrice: 3,
    outputPrice: 15,
    contextWindow: 200000,
    color: '#cc785c',
  },
  {
    id: 'gemini-3-flash-preview',
    name: 'Gemini 3 Flash',
    provider: 'google',
    description: 'ì°¨ì„¸ëŒ€ Gemini. ì´ˆê³ ì† ì¶”ë¡  ë° 1M+ ì»¨í…ìŠ¤íŠ¸.',
    inputPrice: 0.5,
    outputPrice: 1.5,
    contextWindow: 1048576,
    color: '#4285f4',
  },
  {
    id: 'llama-4-maverick',
    name: 'Llama 4 Maverick',
    provider: 'meta',
    description: '400B MoE ì•„í‚¤í…ì²˜. ë°©ëŒ€í•œ ì§€ì‹ê³¼ ì¶”ë¡  ëŠ¥ë ¥.',
    inputPrice: 2,
    outputPrice: 6,
    contextWindow: 256000,
    color: '#0668E1',
  },

  // ============= Premium Models - High Efficiency =============
  {
    id: 'gpt-4o-mini',
    name: 'GPT-4o Mini',
    provider: 'openai',
    description: 'GPT-4o ìˆ˜ì¤€ì˜ ì§€ëŠ¥ì„ ë§¤ìš° ì €ë ´í•˜ê²Œ ì œê³µ.',
    inputPrice: 0.15,
    outputPrice: 0.6,
    contextWindow: 128000,
    color: '#10a37f',
  },
  {
    id: 'deepseek-chat',
    name: 'DeepSeek V3',
    provider: 'deepseek',
    description: 'ê°€ì„±ë¹„ ê°‘. ì½”ë”© ë° í•œêµ­ì–´ ì„±ëŠ¥ì´ ë§¤ìš° ë›°ì–´ë‚¨.',
    inputPrice: 0.14,
    outputPrice: 0.28,
    contextWindow: 64000,
    color: '#5865f2',
  },
  {
    id: 'gemini-2.5-flash-lite',
    name: 'Gemini 2.5 Flash Lite',
    provider: 'google',
    description: 'ê·¹ë„ë¡œ ì €ë ´í•˜ê³  ë¹ ë¦„. ë‹¨ìˆœ ì§ˆì˜ì‘ë‹µì— ì í•©.',
    inputPrice: 0.075,
    outputPrice: 0.3,
    contextWindow: 1048576,
    color: '#4285f4',
  },
  {
    id: 'claude-3-5-haiku',
    name: 'Claude 3.5 Haiku',
    provider: 'anthropic',
    description: 'Claude íŠ¹ìœ ì˜ í†¤ì•¤ë§¤ë„ˆë¥¼ ìœ ì§€í•˜ë©° ì†ë„ í–¥ìƒ.',
    inputPrice: 0.25,
    outputPrice: 1.25,
    contextWindow: 200000,
    color: '#cc785c',
  },

  // ============= Free Models =============
  {
    id: 'google/gemini-2.0-flash-exp:free',
    name: 'Gemini 2.0 Flash (Free)',
    provider: 'google',
    description: 'ê°•ë ¥ ì¶”ì²œ. 1M ê¸´ ë¬¸ë§¥ì²˜ë¦¬ì™€ ë¹ ë¥¸ ì†ë„. ë©€í‹°ëª¨ë‹¬ ì§€ì›.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 1048576,
    color: '#34a853',
  },
  {
    id: 'meta-llama/llama-3.3-70b-instruct:free',
    name: 'Llama 3.3 70B (Free)',
    provider: 'meta',
    description: 'ë²”ìš©ì ìœ¼ë¡œ ê°€ìž¥ ì•ˆì •ì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸. í•œêµ­ì–´ ì²˜ë¦¬ ìš°ìˆ˜.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 128000,
    color: '#0668E1',
  },
  {
    id: 'deepseek/deepseek-r1-0528:free',
    name: 'DeepSeek R1 (Free)',
    provider: 'deepseek',
    description: 'ì¶”ë¡  íŠ¹í™”. CoT(Chain of Thought) ëŠ¥ë ¥ì´ ê°•í™”ëœ ìµœì‹  ëª¨ë¸.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 64000,
    color: '#5865f2',
  },
  {
    id: 'xiaomi/mimo-v2-flash:free',
    name: 'Xiaomi MiMo V2 (Free)',
    provider: 'xiaomi',
    description: 'ëª¨ë°”ì¼/ì—£ì§€ í™˜ê²½ ìµœì í™”. ê°€ë³ê³  ë¹ ë¦„.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 128000,
    color: '#ff6900',
  },
  {
    id: 'qwen/qwen3-coder:free',
    name: 'Qwen 3 Coder (Free)',
    provider: 'alibaba',
    description: 'ì½”ë”© íŠ¹í™”. ì½”ë“œ ìƒì„± ë° ë¶„ì„ì— ìµœì í™”ë¨.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 32000,
    color: '#6366f1',
  },
  {
    id: 'mistralai/devstral-2512:free',
    name: 'Mistral Devstral (Free)',
    provider: 'mistral',
    description: 'ê°œë°œìžìš© ì‹¤í—˜ ëª¨ë¸. ì½”ë”© ë° ê¸°ìˆ ì ì¸ ìž‘ì—…ì— ì í•©.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 32000,
    color: '#fd6f00',
  },
  {
    id: 'openai/gpt-oss-120b:free',
    name: 'GPT-OSS 120B (Free)',
    provider: 'openai',
    description: 'ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸. GPT ê³„ì—´ì˜ ì„±ëŠ¥ì„ ë¬´ë£Œë¡œ ì²´í—˜.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 8192,
    color: '#10a37f',
  },
  {
    id: 'google/gemma-3-27b-it:free',
    name: 'Gemma 3 27B (Free)',
    provider: 'google',
    description: 'êµ¬ê¸€ì˜ ê³ ì„±ëŠ¥ ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸. ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 131072,
    color: '#4285f4',
  },
];

export const SYNTHESIS_MODELS = ['google/gemma-3-27b-it:free', 'google/gemini-2.0-flash-exp:free'];

export const DEFAULT_SYSTEM_PROMPT = `You are a helpful AI assistant. Respond concisely and accurately.`;

export const SYNTHESIS_PROMPT = `You are an expert synthesizer and data analyst. You will receive responses from multiple AI models to the same user question.
Your task is to create a comprehensive synthesis that adds meta-analysis of the model responses.

Structure your response as follows:
1. âœ¨ **Master Synthesis**: A comprehensive final answer that resolves contradictions and provides the most accurate conclusion.
2. ðŸ” **Model Comparison Analysis**:
   - **Similarities (ê³µí†µì )**: Key points that all or most models agreed upon.
   - **Differences (ì°¨ì´ì )**: Unique insights or different perspectives provided by specific models.
3. âš–ï¸ **Conflict & Ratio (ìƒì¶© ì •ë³´ ë° ë¹„ìœ¨)**: If models provide conflicting information, explicitly state the ratio (e.g., "3 out of 5 models (60%) suggest X, while 2 models (40%) suggest Y").
4. ðŸ’¾ **Key Takeaways**: A quick summary of the most critical facts identified across the orchestration.

Ensure the final result is easy to read using Markdown tables, lists, and bold text. The language of the response should match the language of the user's question (default to Korean if unsure).`;

export const getModelById = (id: string): AIModel | undefined => {
  return MODELS.find(model => model.id === id);
};

export const getModelsByProvider = (provider: string): AIModel[] => {
  return MODELS.filter(model => model.provider === provider);
};
