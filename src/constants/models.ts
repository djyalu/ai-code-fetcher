import { AIModel } from '@/types/chat';

export const MODELS: AIModel[] = [
  {
    id: 'gpt-4o',
    name: 'GPT-4o',
    provider: 'openai',
    description: 'OpenAIì˜ ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸',
    inputPrice: 2.5,
    outputPrice: 10,
    contextWindow: 128000,
    color: '#10a37f',
  },
  {
    id: 'gpt-4o-mini',
    name: 'GPT-4o Mini',
    provider: 'openai',
    description: 'ë¹ ë¥´ê³  ê²½ì œì ì¸ GPT-4o ë²„ì „',
    inputPrice: 0.15,
    outputPrice: 0.6,
    contextWindow: 128000,
    color: '#10a37f',
  },
  {
    id: 'claude-3-5-sonnet',
    name: 'Claude 3.5 Sonnet',
    provider: 'anthropic',
    description: 'Anthropicì˜ ê°€ìž¥ ì§€ëŠ¥ì ì¸ ëª¨ë¸',
    inputPrice: 3,
    outputPrice: 15,
    contextWindow: 200000,
    color: '#cc785c',
  },
  {
    id: 'claude-3-5-haiku',
    name: 'Claude 3.5 Haiku',
    provider: 'anthropic',
    description: 'ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ Claude ëª¨ë¸',
    inputPrice: 0.25,
    outputPrice: 1.25,
    contextWindow: 200000,
    color: '#cc785c',
  },
  {
    id: 'gemini-2.0-flash',
    name: 'Gemini 2.0 Flash',
    provider: 'google',
    description: 'Googleì˜ ìµœì‹  ê³ ì† ëª¨ë¸',
    inputPrice: 0.1,
    outputPrice: 0.4,
    contextWindow: 1048576,
    color: '#4285f4',
  },
  {
    id: 'gemini-1.5-pro',
    name: 'Gemini 1.5 Pro',
    provider: 'google',
    description: 'Googleì˜ ê³ ì„±ëŠ¥ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸',
    inputPrice: 1.25,
    outputPrice: 5,
    contextWindow: 2097152,
    color: '#4285f4',
  },
  {
    id: 'deepseek-chat',
    name: 'DeepSeek V3',
    provider: 'deepseek',
    description: 'ì¤‘êµ­ì˜ ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸',
    inputPrice: 0.14,
    outputPrice: 0.28,
    contextWindow: 64000,
    color: '#5865f2',
  },
  {
    id: 'perplexity/sonar',
    name: 'Perplexity Sonar',
    provider: 'perplexity',
    description: 'ìµœì‹  ì •ë³´ ê²€ìƒ‰ì— íŠ¹í™”ëœ ì¶”ë¡  ëª¨ë¸',
    inputPrice: 1,
    outputPrice: 1,
    contextWindow: 128000,
    color: '#20b2aa',
  },
  {
    id: 'perplexity/sonar-deep-research',
    name: 'Sonar Deep Research',
    provider: 'perplexity',
    description: 'ì‹¬ì¸µ í•™ìˆ  ë° íƒì‚¬ ì—°êµ¬ë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ ê²€ìƒ‰ ëª¨ë¸',
    inputPrice: 2,
    outputPrice: 2,
    contextWindow: 128000,
    color: '#008080',
  },
  // Free Research Models
  {
    id: 'google/gemini-2.0-flash-exp:free',
    name: 'Gemini 2.0 Flash (Free)',
    provider: 'google',
    description: 'ìµœê°• ê°€ì„±ë¹„. ê¸´ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì½ê³  ì •ë³´ë¥¼ ì°¾ëŠ” ë° ì••ë„ì  1ìœ„ìž…ë‹ˆë‹¤.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 1048576,
    color: '#34a853',
  },
  {
    id: 'nvidia/llama-3.1-nemotron-70b-instruct:free',
    name: 'Nemotron 70B',
    provider: 'nvidia',
    description: 'ì •ë°€ ì¶”ì¶œ. íŒ©íŠ¸ ì¤‘ì‹¬ì˜ ì •ë³´ ì¶”ì¶œê³¼ ë¹ ë¥¸ ì‘ë‹µ ì†ë„ê°€ ê°•ì ìž…ë‹ˆë‹¤.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 131072,
    color: '#76b900',
  },
  {
    id: 'qwen/qwen-2.5-72b-instruct:free',
    name: 'Qwen 2.5 72B',
    provider: 'qwen',
    description: 'ê¸°ìˆ  ê²€ìƒ‰. ê²€ìƒ‰ ë‚´ìš© ì¤‘ ì½”ë“œê°€ í¬í•¨ë˜ì–´ ìžˆê±°ë‚˜ ê¸°ìˆ ì  ì„¤ëª…ì´ í•„ìš”í•  ë•Œ ì¢‹ìŠµë‹ˆë‹¤.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 131072,
    color: '#615ced',
  },
  {
    id: 'mistralai/mistral-small-3.1-24b-instruct:free',
    name: 'Mistral Small 3.1',
    provider: 'mistral',
    description: 'ì§€ì¹¨ ì¤€ìˆ˜. ì¶œë ¥ í˜•ì‹ì„ ì—„ê²©í•˜ê²Œ ì§€ì¼œì•¼ í•˜ëŠ”(ì˜ˆ: JSON ì¶”ì¶œ) ìž‘ì—…ì— ìœ ë¦¬í•©ë‹ˆë‹¤.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 131072,
    color: '#fd6f00',
  },
  {
    id: 'deepseek/deepseek-chat-v3-0324:free',
    name: 'DeepSeek V3 (Free)',
    provider: 'deepseek',
    description: 'ë¶€ë“œëŸ¬ìš´ ìš”ì•½. ìžì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìž¬êµ¬ì„±í•´ì£¼ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 131072,
    color: '#5865f2',
  },
  {
    id: 'google/gemma-3-27b-it:free',
    name: 'Gemma 3 27B',
    provider: 'google',
    description: 'ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰. í…ìŠ¤íŠ¸ë¿ ì•„ë‹ˆë¼ ì´ë¯¸ì§€ ì† ì •ë³´ë¥¼ ê²€ìƒ‰/ë¶„ì„í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 131072,
    color: '#4285f4',
  },
  {
    id: 'meta-llama/llama-3.3-70b-instruct:free',
    name: 'Llama 3.3 70B',
    provider: 'meta',
    description: 'ë²”ìš©ì„±. ê°€ìž¥ í‘œì¤€ì ì´ê³  ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ëŒ€ê·œëª¨ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ìž…ë‹ˆë‹¤.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 131072,
    color: '#0668E1',
  },
  {
    id: 'microsoft/phi-4:free',
    name: 'Phi-4',
    provider: 'microsoft',
    description: 'ì‹¬ì¸µ ì‚¬ê³ . ë‹µë³€ ì „ ë‚´ë¶€ ì¶”ë¡  ê³¼ì •ì„ ê±°ì³ ë” ê¹Šì´ ìžˆëŠ” ë¶„ì„ ê²°ê³¼ë¥¼ ë‚´ë†“ìŠµë‹ˆë‹¤.',
    inputPrice: 0,
    outputPrice: 0,
    contextWindow: 16384,
    color: '#00bcf2',
  },
];

export const SYNTHESIS_MODELS = ['google/gemini-2.0-flash-exp:free', 'nvidia/llama-3.1-nemotron-70b-instruct:free', 'meta-llama/llama-3.3-70b-instruct:free'];

export const DEFAULT_SYSTEM_PROMPT = `You are a helpful AI assistant. Respond concisely and accurately.`;

export const SYNTHESIS_PROMPT = `You are an expert synthesizer and data analyst. You will receive responses from multiple AI models to the same user question.
Your task is to create a comprehensive synthesis that adds meta-analysis of the model responses.

Structure your response as follows:
1. âœ¨ **Master Synthesis**: A comprehensive final answer that resolves contradictions and provides the most accurate conclusion.
2. ðŸ” **Model Comparison Analysis**:
   - **Similarities (ê³µí†µì )**: Key points that all or most models agreed upon.
   - **Differences (ì°¨ì´ì )**: Unique insights or different perspectives provided by specific models.
3. âš–ï¸ **Conflict & Ratio (ìƒì¶© ì •ë³´ ë° ë¹„ìœ¨)**: If models provide conflicting information, explicitly state the ratio (e.g., "3 out of 5 models (60%) suggest X, while 2 models (40%) suggest Y").
4. ðŸ’¾ **Key Takeaways**: A quick summary of the most critical facts identified across the orchestration.

Ensure the final result is easy to read using Markdown tables, lists, and bold text. The language of the response should match the language of the user's question (default to Korean if unsure).`;

export const getModelById = (id: string): AIModel | undefined => {
  return MODELS.find(model => model.id === id);
};

export const getModelsByProvider = (provider: string): AIModel[] => {
  return MODELS.filter(model => model.provider === provider);
};
